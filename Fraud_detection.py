# -*- coding: utf-8 -*-
"""Fraud Detection : Leveraging Machine Learning for Credit Card Security.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Yvt0bN1NXdKgbPlL9hv53-uE-Fucf6_f
"""

!pip install shap

import pandas as pd
import shap
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import GridSearchCV
from confluent_kafka import Producer, Consumer

data = pd.read_csv('creditcard.csv')

data = data.dropna(subset=['Class'])

X = data.drop('Class', axis=1)
y = data['Class']

data['Hour'] = (data['Time']/ 3600) %24
data['Day'] = (data['Time'] / (3600*24)) % 7

data['Avg_Amount'] = data.groupby('Class')['Amount'].transform('mean')
data['Transaction_Count'] = data.groupby('Class')['Amount'].transform('count')

data['Amount_ratio'] = data['Amount'] / data['Avg_Amount']

data = data.drop(['Time'], axis=1)

X = data.drop('Class', axis=1)
y = data['Class']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.fit_transform(X_test)

smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)

print('original dataset shape:', y_train.value_counts())
print('resampled dataset shape:', y_train_resampled.value_counts())

plt.figure(figsize=(10, 6))
sns.histplot(data['Amount'], bins=50, kde=True)
plt.title('Distribution of Transaction Amounts')
plt.show()

plt.figure(figsize=(6, 4))
sns.countplot(x='Class', data=data)
plt.title('Class Distribution')
plt.show()

class_counts = data['Class'].value_counts()
plt.figure(figsize=(6, 6))
plt.pie(class_counts, labels=class_counts.index, autopct='%1.1f%%', startangle=140, colors=['lightblue', 'salmon'])
plt.title('Class Distribution')
plt.show()

plt.figure(figsize=(20, 15))
for i, col in enumerate(data.columns[1:29]):
  plt.subplot(7, 4, i+1)
  sns.histplot(data[col], bins=50, kde=True)
  plt.title(f'Distribution of {col}')
plt.tight_layout()
plt.show()

plt.figure(figsize=(10, 6))
sns.boxplot(x='Class', y='Amount', data=data)
plt.title('Boxplot of Transaction Amounts by Class')
plt.show()

selected_features = ['V1', 'V2', 'V3', 'V4', 'Amount', 'Class']
sns.pairplot(data[selected_features], hue='Class', diag_kind='kde')
plt.show()

fig = px.scatter(data, x='V1', y='V2', color='Class', title='Scatter Plot of V1 vs V2')
fig.show()

fig = px.scatter_3d(data, x='V1', y='V2', z='V3', color='Class', title='Scatter Plot of V1, V2 and V3')
fig.show()

models = {
    'Logistic Regression': LogisticRegression(),
    'Random Forest': RandomForestClassifier(random_state=42),
    'Gradient Boosting': GradientBoostingClassifier(random_state=42)
}

for name, model in models.items():
  print(f"Training {name}...")

  if name == 'Random Forest':
    model.fit(X_train_resampled, y_train_resampled)
    y_pred = model.predict(X_test_scaled)
  else:
    model.fit(X_train_scaled, y_train)
    y_pred = model.predict(X_test_scaled)

  precision = precision_score(y_test, y_pred)
  recall = recall_score(y_test, y_pred)
  f1 = f1_score(y_test, y_pred)
  roc_auc = roc_auc_score(y_test, y_pred)

  print(f"\n{name} Metrics:")
  print(f"Precision: {precision:.4f}")
  print(f"Recall: {recall:.4f}")
  print(f"F1-score: {f1:.4f}")
  print(f"ROC-AUC: {roc_auc:.4f}\n")

for name, model in models.items():
    print(f"Cross-validation for {name}:")
    scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')  # Change scoring as needed
    print(f"Mean Accuracy: {scores.mean():.4f}")
    print(f"Standard Deviation: {scores.std():.4f}\n")

param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 5, 10],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

rf = RandomForestClassifier(random_state=42)
grid_search = GridSearchCV(rf, param_grid, cv=5, scoring='accuracy')
grid_search.fit(X_train_scaled, y_train)

print("Best Parameters:", grid_search.best_params_)
print("Best Score:", grid_search.best_score_)

X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X.columns)

explainer = shap.Explainer(model, X_train_scaled)

shap_values = explainer.shap_values(X_test_scaled_df)

shap.summary_plot(shap_values, X_test_scaled_df)

shap.force_plot(explainer.expected_value, shap_values[0], X_test_scaled_df.iloc[0])

